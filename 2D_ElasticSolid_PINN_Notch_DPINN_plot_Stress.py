# -*- coding: utf-8 -*-
"""
Created on Wed Sep  1 20:51:50 2021

@author: gaura
"""
# -*- coding: utf-8 -*-
"""2D_ElasticSolid_PINN_Notch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MM493NL9X_d2UVxxdENOWSgEGgVmE0jw
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Importing Libraries

import os
import tensorflow as tf
import numpy as np
import pandas as pd
import math as mh
import matplotlib.pyplot as plt
import scipy.io
import matplotlib.gridspec as gridspec
import time
import sys

# Function to initialize weights and biases for the Neural Network
def Init_NN(neuron_layers):
  weights = []
  biases = []
  n_neuron_layers = len(neuron_layers)

# Loop to create and initialize weights and biases
# loop will run from layer 0 to the last layer.
  for l in range(0,n_neuron_layers-1):
# Creating wieghts and biases here for the layer l (lth iteration of for loop)
# The size of Tensor W will be (number of inputs in layer l X number of outputs in layer l+1)
    W = xavier_init(size=[neuron_layers[l], neuron_layers[l+1]])
# The size of Tensor b will be (1 X number of outputs in layer l+1)
    b = tf.Variable(tf.zeros([1,neuron_layers[l+1]], dtype=tf.float32), dtype=tf.float32)
# Appending the weight Tensor W to weights list for lth iteration
    weights.append(W)
# Appending the bias Tensor b to biases list for lth iteration
    biases.append(b)  
# Returing weights and biases list
  return weights, biases

# Function to perform Xavier Initialization for any given tensor of given size
def xavier_init(size):
    in_dim = size[0]
    out_dim = size[1]        
    xavier_stddev = np.sqrt(2/(in_dim + out_dim))
    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)

# Fucntion to create Neural Network (Returns the final output of the Forward operation)
def neural_net(X, weights, biases):
  num_layers = len(weights) + 1
  H = X
# Loop to create layers of neural-network
# It will create weights and biases from 1st to 2nd-last layer
  for l in range(0,num_layers-2):
# Assigning weights associated with layer l-1 inputs to to tensor W
    W = weights[l]
# Assigning the weights assosocated with l-1 layer bias to tensor b
    b = biases[l]
# Calculating the output of layer l
# The H on the R.H.S. is the input from l-1 layer.
# After doing the matrix operation and non-linearity operation, the 
# output is assigned to the H tensor, which is the value of neurons at layer l
# This H tensor will be used in next iteration as a input 
    H = tf.tanh(tf.add(tf.matmul(H, W), b))

# Assigning weights associated with 2nd -last layer inputs to to tensor W
  W = weights[-1]

# Assigning the weights assosocated with l-1 layer bias to tensor b
  b = biases[-1]

# Calculating final layer output
  Y = tf.add(tf.matmul(H, W), b)
  return Y

# Function to calculate Residual of the govering equation
def Equation_loss(X,weights, biases,C,sx,sy):
  # Cacluating u inside the domain using NN
  U = neural_net(X, weights, biases)
  u = U[:,0]
  v = U[:,1]
  u_grad = tf.gradients(u,X)
  v_grad = tf.gradients(v,X)
  du_dx = (1.0/sx)*u_grad[0][:,0]
  du_dy = (1.0/sy)*u_grad[0][:,1]
  dv_dx = (1.0/sx)*v_grad[0][:,0]
  dv_dy = (1.0/sy)*v_grad[0][:,1]
  exx = du_dx
  eyy = dv_dy 
  exy = du_dy +  dv_dx
 
  sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
  sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
  sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy

  dsig_xx_dx = (1.0/sx)*tf.gradients(sig_xx,X)[0][:,0]
  dsig_xy_dy = (1.0/sy)*tf.gradients(sig_xy,X)[0][:,1]
  dsig_yy_dy = (1.0/sx)*tf.gradients(sig_yy,X)[0][:,1]
  dsig_xy_dx = (1.0/sy)*tf.gradients(sig_xy,X)[0][:,0]

  b1 = 0.0*(-0.2*C[0][0] -0.15* C[0][1] - 0.55*C[2][2])
  b2 = 0.0*(-0.1*C[0][1] -0.2* C[1][1] - 0.2*C[2][2])

  f1 = dsig_xx_dx + dsig_xy_dy + b1
  f2 = dsig_xy_dx + dsig_yy_dy + b2

  return f1,f2

# for box 1
def ExactSolution(X):
    
    nu = 0.3
    k = (3.0-nu)/(1.0+nu)
    lam_I = 0.544483736782464
    r = tf.sqrt(X[:,0]**2 + X[:,1]**2)
    phi = tf.math.atan2(X[:,1],X[:,0])
    Q = 0.543075578836737
    
    u_r = (r**lam_I)*((k-Q)*tf.cos(lam_I*phi)-lam_I*tf.cos((lam_I-2)*phi))
    u_phi = (r**lam_I)*((k+Q)*tf.sin(lam_I*phi)+lam_I*tf.sin((lam_I-2)*phi))
    
    ux = u_r*tf.cos(phi) - u_phi*tf.sin(phi)
    uy = u_r*tf.sin(phi) + u_phi*tf.cos(phi)
    
    U = tf.transpose(tf.concat([[ux],[uy]],axis=0))
    
    
    u_grad = tf.gradients(ux,X)
    v_grad = tf.gradients(uy,X)
    du_dx = u_grad[0][:,0]
    du_dy = u_grad[0][:,1]
    dv_dx = v_grad[0][:,0]
    dv_dy = v_grad[0][:,1]
    exx = du_dx
    eyy = dv_dy 
    exy = du_dy +  dv_dx
    
    C = tf.Variable([[1.0, nu, 0.0],[nu,1.0,0.0],[0.0, 0.0, (1-nu)/2.0]],trainable=False)
 
    sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
    sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
    sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy

    dsig_xx_dx = tf.gradients(sig_xx,X)[0][:,0]
    dsig_xy_dy = tf.gradients(sig_xy,X)[0][:,1]
    dsig_yy_dy = tf.gradients(sig_yy,X)[0][:,1]
    dsig_xy_dx = tf.gradients(sig_xy,X)[0][:,0]

    b1 = 0.0*(-0.2*C[0][0] -0.15* C[0][1] - 0.55*C[2][2])
    b2 = 0.0*(-0.1*C[0][1] -0.2* C[1][1] - 0.2*C[2][2])

    f1 = dsig_xx_dx + dsig_xy_dy + b1
    f2 = dsig_xy_dx + dsig_yy_dy + b2
    
    
    sig = tf.transpose(tf.concat([[sig_xx],[sig_yy],[sig_xy]],axis=0))
  
    return sig

  
    

def StressTensor(X,weights,biases,C,sx,sy):
  # Cacluating u inside the domain using NN
  U = neural_net(X, weights, biases)
  u = U[:,0]
  v = U[:,1]
  u_grad = tf.gradients(u,X)
  v_grad = tf.gradients(v,X)
  du_dx = (1.0/sx)*u_grad[0][:,0]
  du_dy = (1.0/sy)*u_grad[0][:,1]
  dv_dx = (1.0/sx)*v_grad[0][:,0]
  dv_dy = (1.0/sy)*v_grad[0][:,1]
  exx = du_dx
  eyy = dv_dy 
  exy = du_dy +  dv_dx
 
  sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
  sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
  sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy
  
  sig = tf.transpose(tf.concat([[sig_xx],[sig_yy],[sig_xy]],axis=0))
  
  return sig
  


def callback(mse):
  print('Loss,  %.12e' % (mse))

tf.reset_default_graph()
xc = 0.0
yc = 0.0


xmin = -1.0
xmax = 1.0
ymin = -1.0
ymax = 0.0
x = []
y = []
X = []
Y = []


Nx = 51
Ny = 51

dx = (xmax-xmin)/(Nx-1)
dy = (ymax-ymin)/(Ny-1)

for i in range(0,Nx):
    x.append(xmin+i*dx)

for i in range(0,Ny):
    y.append(ymin+i*dy)
    
for i in range(0,Ny):
    for j in range(0,Nx):
        X.append(x[j])
        Y.append(y[i])
    
      


X_T = []
Y_T = []

for i in range(0,len(X)):
    X_T.append(X[i]*np.cos(0.25*np.pi)-Y[i]*np.sin(0.25*np.pi))
    Y_T.append(X[i]*np.sin(0.25*np.pi)+Y[i]*np.cos(0.25*np.pi))


 
#scaling

xmax_tr = max(X_T)
xmin_tr = min(X_T)
ymax_tr = max(Y_T)
ymin_tr = min(Y_T)
tr_1 = [xmax_tr, xmin_tr, ymax_tr, ymin_tr]
sf_x_1 = (xmax_tr-xmin_tr)
sf_y_1 = (ymax_tr-ymin_tr)
X_T_S = []
Y_T_S = []


for i in range(0,len(X)):
    X_T_S.append((X_T[i]-xmin_tr)/(sf_x_1))
    Y_T_S.append((Y_T[i]-ymin_tr)/(sf_y_1))




XY_1 = np.transpose(np.concatenate([[X_T],[Y_T]],axis=0))


XY_S1 = np.transpose(np.concatenate([[X_T_S],[Y_T_S]],axis=0))


D_XY_1 = tf.placeholder(dtype=tf.float32, shape=(None,2), name='D_XY_1')


D_XY_S1 = tf.placeholder(dtype=tf.float32, shape=(None,2), name='D_XY_S1')



#domain 2

xmin = 0.0
xmax = 1.0
ymin = 0.0
ymax = 1.0
x = []
y = []
X = []
Y = []


Nx = 51
Ny = 51

dx = (xmax-xmin)/(Nx-1)
dy = (ymax-ymin)/(Ny-1)

for i in range(0,Nx):
    x.append(xmin+i*dx)

for i in range(0,Ny):
    y.append(ymin+i*dy)
    
for i in range(0,Ny):
    for j in range(0,Nx):
        X.append(x[j])
        Y.append(y[i])
    
        

X_T = []
Y_T = []


for i in range(0,len(X)):
    X_T.append(X[i]*np.cos(0.25*np.pi)-Y[i]*np.sin(0.25*np.pi))
    Y_T.append(X[i]*np.sin(0.25*np.pi)+Y[i]*np.cos(0.25*np.pi))



#scaling
xmax_tr = max(X_T)
xmin_tr = min(X_T)
ymax_tr = max(Y_T)
ymin_tr = min(Y_T)
sf_x_2 = (xmax_tr-xmin_tr)
sf_y_2 = (ymax_tr-ymin_tr)
X_T_S = []
Y_T_S = []


for i in range(0,len(X)):
    X_T_S.append((X_T[i]-xmin_tr)/(sf_x_2))
    Y_T_S.append((Y_T[i]-ymin_tr)/(sf_y_2))



    

XY_2 = np.transpose(np.concatenate([[X_T],[Y_T]],axis=0))


XY_S2 = np.transpose(np.concatenate([[X_T_S],[Y_T_S]],axis=0))


D_XY_2 = tf.placeholder(dtype=tf.float32, shape=(None,2), name='D_XY_2')


D_XY_S2 = tf.placeholder(dtype=tf.float32, shape=(None,2), name='D_XY_S2')




tf_dict_ex_1 = {D_XY_1:XY_1}
tf_dict_nn_1 = {D_XY_S1:XY_S1}
tf_dict_ex_2 = {D_XY_2:XY_2}
tf_dict_nn_2 = {D_XY_S2:XY_S2}

# Architecture of the ANN (we will keep the architecure same for both domains)
# First element is number of inputs
# Last element is number of outputs for each input
# Other elements hold number of neurons in hidden layers
neuron_layers = [2,50,50,50,50,2]

# initializing the weigts and biases of the Neural network
weights_1 = []
biases_1  = [] 
weights_1, biases_1 = Init_NN(neuron_layers)

weights_2 = []
biases_2  = [] 
weights_2, biases_2 = Init_NN(neuron_layers)

nu = 0.3
C = tf.Variable([[1.0, nu, 0.0],[nu,1.0,0.0],[0.0, 0.0, 0.5*(1-nu)]],trainable=False)

pred_sig_1 = StressTensor(D_XY_S1, weights_1, biases_1,C,sf_x_1,sf_y_1)
pred_sig_2 = StressTensor(D_XY_S2, weights_2, biases_2,C,sf_x_2,sf_y_2)
sig_ex_1 = ExactSolution(D_XY_1)
sig_ex_2 = ExactSolution(D_XY_2)

# init will initialize all variables
init = tf.global_variables_initializer()

saver_w_1 = tf.train.Saver(weights_1)
saver_b_1 = tf.train.Saver(biases_1)
saver_w_2 = tf.train.Saver(weights_2)
saver_b_2 = tf.train.Saver(biases_2)

with tf.Session() as sess:
    sess.run(init)
    SIG_EX_1 = sess.run(sig_ex_1, tf_dict_ex_1)
    SIG_EX_2 = sess.run(sig_ex_2, tf_dict_ex_2)
    # Loading optimized weights and biases
    saver_w_1.restore(sess, "./2D_ElasticSolid_Notch_DPINN/my_training_mode_nn1_w1.ckpt")
    saver_b_1.restore(sess, "./2D_ElasticSolid_Notch_DPINN/my_training_mode_nn1_b1.ckpt")
    saver_w_2.restore(sess, "./2D_ElasticSolid_Notch_DPINN/my_training_mode_nn1_w2.ckpt")
    saver_b_2.restore(sess, "./2D_ElasticSolid_Notch_DPINN/my_training_mode_nn1_b2.ckpt")
    SIG_NN_1 = sess.run(pred_sig_1, tf_dict_nn_1)
    SIG_NN_2 = sess.run(pred_sig_2, tf_dict_nn_2)
sess.close()

XY = np.concatenate([XY_1,XY_2], axis=0)
Npts = XY.shape[0]
x = np.reshape(XY[:,0],[Npts,1])
y = np.reshape(XY[:,1],[Npts,1])

sig_xx_pred_1 = SIG_NN_1[:,0]
sig_yy_pred_1 = SIG_NN_1[:,1]
sig_xx_ex_1 = SIG_EX_1[:,0]
sig_yy_ex_1 = SIG_EX_1[:,1]

sig_xx_pred_2 = SIG_NN_2[:,0]
sig_yy_pred_2 = SIG_NN_2[:,1]
sig_xx_ex_2 = SIG_EX_2[:,0]
sig_yy_ex_2 = SIG_EX_2[:,1]

# print(x.shape)
# print(y.shape)


sig_xx_pred = np.transpose(np.concatenate([[sig_xx_pred_1],[sig_xx_pred_2]],axis=1))
sig_yy_pred = np.transpose(np.concatenate([[sig_yy_pred_1],[sig_yy_pred_2]],axis=1))

sig_xx_ex = np.transpose(np.concatenate([[sig_xx_ex_1],[sig_xx_ex_2]],axis=1))
sig_yy_ex = np.transpose(np.concatenate([[sig_yy_ex_1],[sig_yy_ex_2]],axis=1))

'''
ERR_IN_UR = np.abs(u_ex-u_pred)
ERR_IN_UTheta = np.abs(v_ex-v_pred)
ERR_PDE_1 = np.reshape(np.concatenate([pde_1_1_err,pde_1_2_err],axis=0),[Npts,1])
ERR_PDE_2 = np.reshape(np.concatenate([pde_2_1_err,pde_2_2_err],axis=0),[Npts,1])
'''
# print(ERR_PDE_1.shape)
# print(ERR_PDE_2.shape)
# print(u_pred.shape)
# print(v_pred.shape)
# print(u_ex.shape)
# print(v_ex.shape)

fig1, uplot_fit = plt.subplots()
C1 = uplot_fit.scatter(x,y,marker ='.',c=sig_xx_ex)
plt.colorbar(C1)

uplot_fit.set_title('Contours of sig_xx (Exact)')

fig2, uplot_fit = plt.subplots()
C2 = uplot_fit.scatter(x,y,marker ='.',c=sig_xx_pred)
plt.colorbar(C2)
uplot_fit.set_title('Contours of sig_xx (PINN)')
'''
Data_ur = np.concatenate((x,y,u_pred,u_ex,ERR_IN_UR,ERR_PDE_1),axis = 1 )
Data_u_theta = np.concatenate((x,y,v_pred,v_ex,ERR_IN_UTheta,ERR_PDE_2),axis = 1)
np.savetxt("./2D_ElasticSolid_Notch_DPINN/2D_Notch_PST_ur_ERR_Train.csv", Data_ur, delimiter=',', header="x,y,U_R_Predicted,U_R_Analytical, U_R_ERR, PDE_1_ERR", comments="")
np.savetxt("./2D_ElasticSolid_Notch_DPINN/2D_Notch_PST_u_theta_ERR_Train.csv", Data_u_theta, delimiter=',', header="x,y,U_Theta_Predicted,U_Theta_Analytical,U_THETA_ERR, PDE_2_ERR", comments="")
'''