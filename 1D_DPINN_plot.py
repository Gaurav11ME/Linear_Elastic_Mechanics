# -*- coding: utf-8 -*-
"""
Created on Wed Sep  1 13:36:54 2021

@author: gaura
"""
# -*- coding: utf-8 -*-
"""1D_ElasticSolid_PINN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWTLPrbeRdDOtot13SGJlogkLNCPW5hr
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Importing Libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import math as mh
import matplotlib.pyplot as plt
import scipy.io
import matplotlib.gridspec as gridspec
import time

# Function to initialize weights and biases for the Neural Network
def Init_NN(neuron_layers):
  weights = []
  biases = []
  n_neuron_layers = len(neuron_layers)

# Loop to create and initialize weights and biases
# loop will run from layer 0 to the last layer.
  for l in range(0,n_neuron_layers-1):
# Creating wieghts and biases here for the layer l (lth iteration of for loop)
# The size of Tensor W will be (number of inputs in layer l X number of outputs in layer l+1)
    W = xavier_init(size=[neuron_layers[l], neuron_layers[l+1]])
# The size of Tensor b will be (1 X number of outputs in layer l+1)
    b = tf.Variable(tf.zeros([1,neuron_layers[l+1]], dtype=tf.float32), dtype=tf.float32)
# Appending the weight Tensor W to weights list for lth iteration
    weights.append(W)
# Appending the bias Tensor b to biases list for lth iteration
    biases.append(b)  
# Returing weights and biases list
  return weights, biases

# Function to perform Xavier Initialization for any given tensor of given size
def xavier_init(size):
    in_dim = size[0]
    out_dim = size[1]        
    xavier_stddev = np.sqrt(2/(in_dim + out_dim))
    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)

# Fucntion to create Neural Network (Returns the final output of the Forward operation)
def neural_net(X, weights, biases):
  num_layers = len(weights) + 1
  H = X
# Loop to create layers of neural-network
# It will create weights and biases from 1st to 2nd-last layer
  for l in range(0,num_layers-2):
# Assigning weights associated with layer l-1 inputs to to tensor W
    W = weights[l]
# Assigning the weights assosocated with l-1 layer bias to tensor b
    b = biases[l]
# Calculating the output of layer l
# The H on the R.H.S. is the input from l-1 layer.
# After doing the matrix operation and non-linearity operation, the 
# output is assigned to the H tensor, which is the value of neurons at layer l
# This H tensor will be used in next iteration as a input 
    H = tf.tanh(tf.add(tf.matmul(H, W), b))

# Assigning weights associated with 2nd -last layer inputs to to tensor W
  W = weights[-1]

# Assigning the weights assosocated with l-1 layer bias to tensor b
  b = biases[-1]

# Calculating final layer output
  Y = tf.add(tf.matmul(H, W), b)
  return Y

# Function to calculate Residual of the govering equation
def Equation_loss(X, weights, biases,k):
  # Cacluating u inside the domain using NN
  u = neural_net(X, weights, biases)

  # Calculating f inside the domain
  du_dx = tf.gradients(u,X)[0]
  q = -k*du_dx
  Lu = -tf.gradients(q,X)[0]
  qx = X**3;
  f = Lu+qx; 

  return f,q

# Inference
# Creating inference grid
tf.reset_default_graph()
xl_test = []
xr_test = []

# k values
k1 = 1.0;
k2 = 10.0;

# number of testing points on each domain
n_testPoints = 42

xl = 0.0
xr = 1.0;
xm = 0.5;

dx1 = (xm-xl)/(n_testPoints-1)
dx2 = (xr-xm)/(n_testPoints-1)

xl_test.append([xl])
for i in range(2,n_testPoints):
  xl_test.append([xl + (i-1)*(dx1)])
xl_test.append([xm])

ul_th =[]
for i in range(0,n_testPoints):
  y = xl_test[i][0]
  ul_th.append([(-y/320.0)*((16*y**4)*(k1+k2)-31*k1-k2)*(1.0/(k1*(k1+k2)))])


xr_test.append([xm])
for i in range(2,n_testPoints):
  xr_test.append([xm + (i-1)*(dx2)])
xr_test.append([xr])

ur_th = []
for i in range(0,n_testPoints):
  y = xr_test[i][0]
  ur_th.append([(-1/320.0)*((16*y**5)*(k1+k2)-31*k1*y-k2*y+15*(k1-k2))*(1.0/(k2*(k1+k2)))])

XL = tf.placeholder(dtype=tf.float32, shape=(None,1), name='XL')
XR = tf.placeholder(dtype=tf.float32, shape=(None,1), name='XR')

tf_dict_L = {XL:xl_test}
tf_dict_R = {XR:xr_test}

# Architecture of the ANN (we will keep the architecure same for both domains)
# First element is number of inputs
# Last element is number of outputs
# Other elements hold number of neurons in hidden layers
neuron_layers = [1,5,5,1]

weights_L = []
biases_L  = [] 
weights_L, biases_L = Init_NN(neuron_layers)

# For domain 2
weights_R = []
biases_R  = [] 
weights_R, biases_R = Init_NN(neuron_layers)

u_L = neural_net(XL,weights_L,biases_L)
u_R = neural_net(XR,weights_R,biases_R)

# init will initialize all variables
init = tf.global_variables_initializer()

# Creating a saver object
saver_1 = tf.train.Saver(weights_L)
saver_2 = tf.train.Saver(biases_L)
saver_3 = tf.train.Saver(weights_R)
saver_4 = tf.train.Saver(biases_R)

epoch = 0

with tf.Session() as sess:
  sess.run(init)
  # loading optimized weights and biases
  saver_1.restore(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_w1.ckpt")
  saver_2.restore(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_b1.ckpt")
  saver_3.restore(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_w2.ckpt")
  saver_4.restore(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_b2.ckpt")
  UL = sess.run(u_L,tf_dict_L)
  UR = sess.run(u_R,tf_dict_R)
sess.close()

xl_test = np.array(xl_test)
xr_test = np.array(xr_test)
X_D = np.concatenate((xl_test, xr_test),axis=0)
U_D_NN = np.concatenate((UL, UR),axis=0)
U_D_TH = np.concatenate((ul_th, ur_th),axis=0)

L2_ERR = 0.0
Area = 1.0;
D_area = Area/(2*n_testPoints)

for i in range(0,X_D.shape[0]):
    L2_ERR = L2_ERR + (U_D_NN[i][0]-U_D_TH[i][0])**2*D_area 

L2_ERR = np.sqrt(L2_ERR)
print(L2_ERR)
Data = np.concatenate((X_D, U_D_NN, U_D_TH),axis=1)
np.savetxt("./1D_ElasticSolid/1D_DPINN_Train.csv", Data, delimiter=',', header="X,U_Predicted,U_Analytical", comments="")

plt.plot(xl_test,UL,".k",markevery=2)
plt.plot(xl_test,ul_th,'b')
plt.plot(xr_test,ur_th,'r')
plt.plot(xr_test,UR,'.k',markevery=2)
plt.xlabel('x')
plt.ylabel('u')