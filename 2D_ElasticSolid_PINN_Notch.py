# -*- coding: utf-8 -*-
"""2D_ElasticSolid_PINN_Notch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MM493NL9X_d2UVxxdENOWSgEGgVmE0jw
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Importing Libraries

import os
import tensorflow as tf
import numpy as np
import pandas as pd
import math as mh
import matplotlib.pyplot as plt
import scipy.io
import matplotlib.gridspec as gridspec
import time
import sys

# Function to initialize weights and biases for the Neural Network
def Init_NN(neuron_layers):
  weights = []
  biases = []
  n_neuron_layers = len(neuron_layers)

# Loop to create and initialize weights and biases
# loop will run from layer 0 to the last layer.
  for l in range(0,n_neuron_layers-1):
# Creating wieghts and biases here for the layer l (lth iteration of for loop)
# The size of Tensor W will be (number of inputs in layer l X number of outputs in layer l+1)
    W = xavier_init(size=[neuron_layers[l], neuron_layers[l+1]])
# The size of Tensor b will be (1 X number of outputs in layer l+1)
    b = tf.Variable(tf.zeros([1,neuron_layers[l+1]], dtype=tf.float32), dtype=tf.float32)
# Appending the weight Tensor W to weights list for lth iteration
    weights.append(W)
# Appending the bias Tensor b to biases list for lth iteration
    biases.append(b)  
# Returing weights and biases list
  return weights, biases

# Function to perform Xavier Initialization for any given tensor of given size
def xavier_init(size):
    in_dim = size[0]
    out_dim = size[1]        
    xavier_stddev = np.sqrt(2/(in_dim + out_dim))
    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)

# Fucntion to create Neural Network (Returns the final output of the Forward operation)
def neural_net(X, weights, biases):
  num_layers = len(weights) + 1
  H = X
# Loop to create layers of neural-network
# It will create weights and biases from 1st to 2nd-last layer
  for l in range(0,num_layers-2):
# Assigning weights associated with layer l-1 inputs to to tensor W
    W = weights[l]
# Assigning the weights assosocated with l-1 layer bias to tensor b
    b = biases[l]
# Calculating the output of layer l
# The H on the R.H.S. is the input from l-1 layer.
# After doing the matrix operation and non-linearity operation, the 
# output is assigned to the H tensor, which is the value of neurons at layer l
# This H tensor will be used in next iteration as a input 
    H = tf.tanh(tf.add(tf.matmul(H, W), b))

# Assigning weights associated with 2nd -last layer inputs to to tensor W
  W = weights[-1]

# Assigning the weights assosocated with l-1 layer bias to tensor b
  b = biases[-1]

# Calculating final layer output
  Y = tf.add(tf.matmul(H, W), b)
  return Y

# Function to calculate Residual of the govering equation
def Equation_loss(X,weights, biases,C):
  # Cacluating u inside the domain using NN
  U = neural_net(X, weights, biases)
  u = U[:,0]
  v = U[:,1]
  u_grad = tf.gradients(u,X)
  v_grad = tf.gradients(v,X)
  du_dx = u_grad[0][:,0]
  du_dy = u_grad[0][:,1]
  dv_dx = v_grad[0][:,0]
  dv_dy = v_grad[0][:,1]
  exx = du_dx
  eyy = dv_dy 
  exy = du_dy +  dv_dx
 
  sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
  sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
  sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy

  dsig_xx_dx = tf.gradients(sig_xx,X)[0][:,0]
  dsig_xy_dy = tf.gradients(sig_xy,X)[0][:,1]
  dsig_yy_dy = tf.gradients(sig_yy,X)[0][:,1]
  dsig_xy_dx = tf.gradients(sig_xy,X)[0][:,0]

  b1 = 0.0*(-0.2*C[0][0] -0.15* C[0][1] - 0.55*C[2][2])
  b2 = 0.0*(-0.1*C[0][1] -0.2* C[1][1] - 0.2*C[2][2])

  f1 = dsig_xx_dx + dsig_xy_dy + b1
  f2 = dsig_xy_dx + dsig_yy_dy + b2

  return f1,f2

# for box 1
def ExactSolution(X):
    
    nu = 0.3
    k = (3.0-nu)/(1.0+nu)
    lam_I = 0.544483736782464
    r = X[:,0]**2 + X[:,1]**2
    phi = tf.math.atan2(X[:,1],X[:,0])
    Q = 0.543075578836737
    
    u_r = (r**lam_I)*((k-Q)*tf.cos(lam_I*phi)-lam_I*tf.cos((lam_I-2)*phi))
    u_phi = (r**lam_I)*((k+Q)*tf.sin(lam_I*phi)+lam_I*tf.sin((lam_I-2)*phi))
    
    ux = u_r*tf.cos(phi) - u_phi*tf.sin(phi)
    uy = u_r*tf.sin(phi) + u_phi*tf.cos(phi)
    
    U = tf.transpose(tf.concat([[ux],[uy]],axis=0))
    
    '''
    u_grad = tf.gradients(ux,X)
    v_grad = tf.gradients(uy,X)
    du_dx = u_grad[0][:,0]
    du_dy = u_grad[0][:,1]
    dv_dx = v_grad[0][:,0]
    dv_dy = v_grad[0][:,1]
    exx = du_dx
    eyy = dv_dy 
    exy = du_dy +  dv_dx
    
    C = tf.Variable([[1.0, nu, 0.0],[nu,1.0,0.0],[0.0, 0.0, (1-nu)/2.0]],trainable=False)
 
    sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
    sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
    sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy

    dsig_xx_dx = tf.gradients(sig_xx,X)[0][:,0]
    dsig_xy_dy = tf.gradients(sig_xy,X)[0][:,1]
    dsig_yy_dy = tf.gradients(sig_yy,X)[0][:,1]
    dsig_xy_dx = tf.gradients(sig_xy,X)[0][:,0]

    b1 = 0.0*(-0.2*C[0][0] -0.15* C[0][1] - 0.55*C[2][2])
    b2 = 0.0*(-0.1*C[0][1] -0.2* C[1][1] - 0.2*C[2][2])

    f1 = dsig_xx_dx + dsig_xy_dy + b1
    f2 = dsig_xy_dx + dsig_yy_dy + b2
    '''

    return U
    

def StressTensor(X,weights,biases,C):
  U = neural_net(X, weights, biases)
  u = U[:,0]
  v = U[:,1]
  u_grad = tf.gradients(u,X)
  v_grad = tf.gradients(v,X)
  du_dx = u_grad[0][:,0]
  du_dy = u_grad[0][:,1]
  dv_dx = v_grad[0][:,0]
  dv_dy = v_grad[0][:,1]
  exx = du_dx
  eyy = dv_dy 
  exy = du_dy +  dv_dx
 
  sig_xx = C[0][0]*exx + C[0][1]*eyy+C[0][2]*exy
  sig_yy = C[1][0]*exx + C[1][1]*eyy+C[1][2]*exy
  sig_xy = C[2][0]*exx + C[2][1]*eyy+C[2][2]*exy
    
  return sig_xx, sig_yy, sig_xy


def callback(mse):
  print('Loss,  %.12e' % (mse))

tf.reset_default_graph()

# Domain point Generation
# Domain 1


xc = 0.0
yc = 0.0
XB = []
YB = []
XN = []
YN = []


xmin = -1.0
xmax = 1.0
ymin = -1.0
ymax = 0.0
x = []
y = []
X = []
Y = []


Nx = 51
Ny = 51

dx = (xmax-xmin)/(Nx-1)
dy = (ymax-ymin)/(Ny-1)

for i in range(0,Nx):
    x.append(xmin+i*dx)

for i in range(0,Ny):
    y.append(ymin+i*dy)
    
for i in range(0,Ny):
    for j in range(0,Nx):
        X.append(x[j])
        Y.append(y[i])
    
        if(x[j]==xmin or x[j]==xmax or y[i]==ymin):
            XB.append(x[j])
            YB.append(y[i])
            
        if((x[j]>=xmin and x[j]<=xc) and y[i]==ymax):
            XN.append(x[j])
            YN.append(y[i])


xmin = 0.0
xmax = 1.0
ymin = 0.0
ymax = 1.0
x = []
y = []

Nx = 51
Ny = 51

dx = (xmax-xmin)/(Nx-1)
dy = (ymax-ymin)/(Ny-1)

for i in range(0,Nx):
    x.append(xmin+i*dx)

for i in range(0,Ny):
    y.append(ymin+i*dy)
    
for i in range(0,Ny):
    for j in range(0,Nx):
        X.append(x[j])
        Y.append(y[i])
    
        if(x[j]==xmax or y[i]==ymax):
            XB.append(x[j])
            YB.append(y[i])
            
        if(x[j]==xmin and (y[i]>=ymin and y[i]<=ymax)):
            XN.append(x[j])
            YN.append(y[i])



X_T = []
Y_T = []
XB_T = []
YB_T = []
XN_T = []
YN_T = []


for i in range(0,len(X)):
    X_T.append(X[i]*np.cos(0.25*np.pi)-Y[i]*np.sin(0.25*np.pi))
    Y_T.append(X[i]*np.sin(0.25*np.pi)+Y[i]*np.cos(0.25*np.pi))


for i in range(0,len(XB)):
    XB_T.append(XB[i]*np.cos(0.25*np.pi)-YB[i]*np.sin(0.25*np.pi))
    YB_T.append(XB[i]*np.sin(0.25*np.pi)+YB[i]*np.cos(0.25*np.pi))
 

for i in range(0,len(XN)):
    XN_T.append(XN[i]*np.cos(0.25*np.pi)-YN[i]*np.sin(0.25*np.pi))
    YN_T.append(XN[i]*np.sin(0.25*np.pi)+YN[i]*np.cos(0.25*np.pi)) 

XY = np.transpose(np.concatenate([[X_T],[Y_T]],axis=0))
XY_B = np.transpose(np.concatenate([[XB_T],[YB_T]],axis=0))
XY_C = np.transpose(np.concatenate([[XN_T],[YN_T]],axis=0))

D_XY = tf.placeholder(dtype=tf.float32, shape=(None,2), name='D_XY')
B_XY = tf.placeholder(dtype=tf.float32, shape=(None,2), name='B_XY')
C_XY = tf.placeholder(dtype=tf.float32, shape=(None,2), name='C_XY')

tf_dict = {D_XY:XY, B_XY:XY_B, C_XY:XY_C}

# Number of Epochs
n_epochs = 50000*2;

# Training Rate
train_rate = 0.001;


# Architecture of the ANN (we will keep the architecure same for both domains)
# First element is number of inputs
# Last element is number of outputs for each input
# Other elements hold number of neurons in hidden layers
neuron_layers = [2,50,50,50,50,50,50,2]

# initializing the weigts and biases of the Neural network
weights = []
biases  = [] 
weights, biases = Init_NN(neuron_layers)

nu = 0.3
C = tf.Variable([[1.0, nu, 0.0],[nu,1.0,0.0],[0.0, 0.0, (1-nu)/2.0]],trainable=False)
# Calculating losses
# Equation Loss
[f1,f2] = Equation_loss(D_XY,weights, biases,C)

# BC Loss
UB_Ex = ExactSolution(B_XY)
ux_b_ex = UB_Ex[:,0]
uy_b_ex = UB_Ex[:,1]

UB_NN = neural_net(B_XY, weights, biases)
ux_b_nn = UB_NN[:,0]
uy_b_nn = UB_NN[:,1]

ux_b_loss = ux_b_ex-ux_b_nn
uy_b_loss = uy_b_ex-uy_b_nn

# Traction Free Loss
[sig_xx_trf,sig_yy_trf,sig_xy_trf] = StressTensor(C_XY, weights, biases, C)

mse_PDE = tf.reduce_mean(tf.square(f1))+\
          tf.reduce_mean(tf.square(f2))

mse_BC = tf.reduce_mean(tf.square(ux_b_loss))+\
         tf.reduce_mean(tf.square(uy_b_loss))

mse_TRF = tf.reduce_mean(tf.square(sig_yy_trf))+\
          tf.reduce_mean(tf.square(sig_xy_trf))
         
mse = mse_PDE + mse_BC + mse_TRF

# Prescribing optimizer with a training rate
optimizer = tf.train.AdamOptimizer(learning_rate = train_rate)


# Creating a training operation
training_op = optimizer.minimize(mse)

optimizer_lbfgs_mse = tf.contrib.opt.ScipyOptimizerInterface(mse, method = 'L-BFGS-B', 
                                                  options = {'maxiter': 100000,
                                                  'maxfun': 200000,
                                                  'maxcor': 50,
                                                  'maxls': 50,
                                                  'ftol' : 1.e-16 })

# init will initialize all variables
init = tf.global_variables_initializer()

saver_1 = tf.train.Saver(weights)
saver_2 = tf.train.Saver(biases)

# setting initial value of the epoch
epoch = 0

with tf.Session() as sess:
  sess.run(init)
  #print(sess.run(U,tf_dict))
  #print(sess.run(vb_loss,tf_dict))
 #Loop runs till mse reaches 1e-05 or epoch is less than total epochs
  
  Iterations_VS_MSE = []
  while(epoch<=n_epochs and mse.eval(tf_dict)>1e-08):
    
    sess.run(training_op,tf_dict);
    MSE = mse.eval(tf_dict)
 # Iterations_VS_MSE will record the MSE obtained in each iteration of the while loop,
 # Based on the updated weights and biases in that iteration.   
    Iterations_VS_MSE.append([epoch,MSE])
    
    if(epoch%100==0):
      print("Epoch = ", epoch, " " "MSE = ", mse.eval(tf_dict))

    epoch = epoch+1
  
  original_stdout = sys.stdout  
  sys.stdout = open('L_BFGS_B_Iterations_2D_Notch.csv', 'w') 
 # The weights from the las iteration of the while loop will be used as the initial point to start L-BFGS-B  
  optimizer_lbfgs_mse.minimize(sess,
                     feed_dict = tf_dict,
                     fetches = [mse],
                     loss_callback = callback)
  sys.stdout.close()
  sys.stdout=original_stdout
  
  # Writing the MSE for all iterations after the while loop is finished 
  Iteration_MSE_Data = np.array(Iterations_VS_MSE)
  np.savetxt("2D_PINN_Notch_Iteration_Data.csv", Iteration_MSE_Data, delimiter=',', header="Epoch, MSE", comments="")
  
    
   
  save_path_1 = saver_1.save(sess, "./2D_ElasticSolid_Notch/my_training_mode_nn1_w.ckpt")
  save_path_2 = saver_2.save(sess, "./2D_ElasticSolid_Notch/my_training_mode_nn1_b.ckpt")
  
sess.close()
