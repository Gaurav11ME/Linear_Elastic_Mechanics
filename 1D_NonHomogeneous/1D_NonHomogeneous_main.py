# -*- coding: utf-8 -*-
"""1D_ElasticSolid_PINN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWTLPrbeRdDOtot13SGJlogkLNCPW5hr
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Importing Libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import math as mh
import matplotlib.pyplot as plt
import scipy.io
import matplotlib.gridspec as gridspec
import time

# Function to initialize weights and biases for the Neural Network
def Init_NN(neuron_layers):
  weights = []
  biases = []
  n_neuron_layers = len(neuron_layers)

# Loop to create and initialize weights and biases
# loop will run from layer 0 to the last layer.
  for l in range(0,n_neuron_layers-1):
# Creating wieghts and biases here for the layer l (lth iteration of for loop)
# The size of Tensor W will be (number of inputs in layer l X number of outputs in layer l+1)
    W = xavier_init(size=[neuron_layers[l], neuron_layers[l+1]])
# The size of Tensor b will be (1 X number of outputs in layer l+1)
    b = tf.Variable(tf.zeros([1,neuron_layers[l+1]], dtype=tf.float32), dtype=tf.float32)
# Appending the weight Tensor W to weights list for lth iteration
    weights.append(W)
# Appending the bias Tensor b to biases list for lth iteration
    biases.append(b)  
# Returing weights and biases list
  return weights, biases

# Function to perform Xavier Initialization for any given tensor of given size
def xavier_init(size):
    in_dim = size[0]
    out_dim = size[1]        
    xavier_stddev = np.sqrt(2/(in_dim + out_dim))
    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)

# Fucntion to create Neural Network (Returns the final output of the Forward operation)
def neural_net(X, weights, biases):
  num_layers = len(weights) + 1
  H = X
# Loop to create layers of neural-network
# It will create weights and biases from 1st to 2nd-last layer
  for l in range(0,num_layers-2):
# Assigning weights associated with layer l-1 inputs to to tensor W
    W = weights[l]
# Assigning the weights assosocated with l-1 layer bias to tensor b
    b = biases[l]
# Calculating the output of layer l
# The H on the R.H.S. is the input from l-1 layer.
# After doing the matrix operation and non-linearity operation, the 
# output is assigned to the H tensor, which is the value of neurons at layer l
# This H tensor will be used in next iteration as a input 
    H = tf.tanh(tf.add(tf.matmul(H, W), b))

# Assigning weights associated with 2nd -last layer inputs to to tensor W
  W = weights[-1]

# Assigning the weights assosocated with l-1 layer bias to tensor b
  b = biases[-1]

# Calculating final layer output
  Y = tf.add(tf.matmul(H, W), b)
  return Y

# Function to calculate Residual of the govering equation
def Equation_loss(X, weights, biases,k):
  # Cacluating u inside the domain using NN
  u = neural_net(X, weights, biases)

  # Calculating f inside the domain
  du_dx = tf.gradients(u,X)[0]
  q = -k*du_dx
  Lu = -tf.gradients(q,X)[0]
  qx = X**3;
  f = Lu+qx; 

  return f,q

# list to hold coordinates 
tf.reset_default_graph()
x_L = []
x_R = []

# Left Extent of the domain
xl = 0;

# Right Extent of the domain
xr = 1;

# Location of discontinuity
xm = 0.5

# k in left domain
k1 = 1.0

# k in right domain
k2 = 10.0

# Left boundary value of u
ul  = 0.0

# Right boundary value of u
ur = 0.0

# Number of Epochs
n_epochs = 10000*5;

# Training Rate
train_rate = 0.01;

# number of training points in each domain
n_trainPoints = 42;

# Architecture of the ANN (we will keep the architecure same for both domains)
# First element is number of inputs
# Last element is number of outputs
# Other elements hold number of neurons in hidden layers
neuron_layers = [1,5,5,1]

dx1 = (xm-xl)/(n_trainPoints-1)
dx2 = (xr-xm)/(n_trainPoints-1)

x_L.append([xl])
for i in range(2,n_trainPoints):
  x_L.append([xl + (i-1)*(dx1)])
x_L.append([xm])

x_R.append([xm])
for i in range(2,n_trainPoints):
  x_R.append([xm + (i-1)*(dx2)])
x_R.append([xr])

# initializing the weigts and biases of the Neural network
# For domain 1
weights_L = []
biases_L  = [] 
weights_L, biases_L = Init_NN(neuron_layers)

# For domain 2
weights_R = []
biases_R  = [] 
weights_R, biases_R = Init_NN(neuron_layers)

# Creaintng placeholders, placeholder will feed value to the graph at runtime  
XL = tf.placeholder(dtype=tf.float32, shape=(None,1), name='XL')
XR = tf.placeholder(dtype=tf.float32, shape=(None,1), name='XR')
x_lb = tf.placeholder(dtype=tf.float32, shape=(None,1), name='x_lb')
x_m = tf.placeholder(dtype=tf.float32, shape=(None,1), name='x_m')
x_rb = tf.placeholder(dtype=tf.float32, shape=(None,1), name='x_rb')
u_lb = tf.placeholder(dtype=tf.float32, shape=(None,1), name='u_lb')
u_rb = tf.placeholder(dtype=tf.float32, shape=(None,1), name='u_rb')

tf_dict = {XL:x_L, XR:x_R, x_lb:[[xl]], x_m:[[xm]], x_rb:[[xr]], u_lb:[[ul]], u_rb:[[ur]]}

# Calculating loss in the govering equation
# For Left Side
[f_L,q_L] = Equation_loss(XL,weights_L, biases_L,k1)

# For Right Side
[f_R,q_R] = Equation_loss(XR,weights_R, biases_R,k2)

# Calculate boundary value losses
LB_Loss = neural_net(x_lb, weights_L, biases_L)-u_lb
RB_Loss = neural_net(x_rb, weights_R, biases_R)-u_rb

# Calculate midpoint continuity loss
C_m1 = neural_net(x_m, weights_L, biases_L)-neural_net(x_m, weights_R, biases_R)

# Calculating flux-continuity loss
C_m2 = q_L[len(x_L)-1]-q_R[0]


sf = 1
#n_trainPoints

# Will evaluate MSE
mse = tf.reduce_mean(tf.square(f_L))+\
      tf.reduce_mean(tf.square(f_R))+\
      sf*tf.reduce_mean(tf.square(LB_Loss))+\
      +sf*tf.reduce_mean(tf.square(C_m1))+\
      +sf*tf.reduce_mean(tf.square(C_m2))+\
      +sf*tf.reduce_mean(tf.square(RB_Loss))

# Prescribing optimizer with a training rate
optimizer = tf.train.AdamOptimizer(learning_rate = train_rate)

# Creating a training operation
training_op = optimizer.minimize(mse)

# init will initialize all variables
init = tf.global_variables_initializer()

# Creating a saver object
saver_1 = tf.train.Saver(weights_L)
saver_2 = tf.train.Saver(biases_L)
saver_3 = tf.train.Saver(weights_R)
saver_4 = tf.train.Saver(biases_R)

epoch = 0

with tf.Session() as sess:
  sess.run(init)
 #Loop runs till mse reaches 1e-05 or epoch is less than total epochs
  while(epoch<=n_epochs and mse.eval(tf_dict)>1e-06):
    sess.run(training_op,tf_dict);
    if(epoch%100==0):
      print("Epohch = ", epoch, " " "MSE = ", mse.eval(tf_dict))
    epoch = epoch+1
  

  save_path_1 = saver_1.save(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_w1.ckpt")
  save_path_2 = saver_2.save(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_b1.ckpt")
  save_path_3 = saver_3.save(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_w2.ckpt")
  save_path_4 = saver_4.save(sess, "./1D_ElasticSolid/N84/my_training_mode_nn1_b2.ckpt")
sess.close()

